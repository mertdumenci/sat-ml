{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MG3W5jUd5jK"
   },
   "source": [
    "# Data\n",
    "\n",
    "Let's load and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "FkG8hdhjXAWB",
    "outputId": "bf4a4982-3812-4506-85da-bda850716d56"
   },
   "outputs": [],
   "source": [
    "# # Filesystem, etc\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !ls /content/drive\n",
    "\n",
    "# !git clone https://github.com/mertdumenci/sat-ml.git\n",
    "# !cd sat-ml && git checkout md/fall-2019 && git pull origin md/fall-2019\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('sat-ml')\n",
    "\n",
    "# !pip3 install numpy torch matplotlib\n",
    "\n",
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from satml import expression, fast_cnf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EhDcCoURxg19",
    "outputId": "1d6558e2-e857-41d7-d85e-17d848365a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 606136 decisions\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "DATA_FILE = 'data/sr_20_batch740000_formulas606136.pickle'\n",
    "# DATA_FILE = 'data/toy_batch0_formulas27.pickle'\n",
    "\n",
    "with open(DATA_FILE, 'rb') as f:\n",
    "    decisions = pickle.load(f)\n",
    "\n",
    "assert decisions is not None\n",
    "\n",
    "# Extract the maximum number of variables.\n",
    "len_vocab = max((max((max(c) for c in formula)) for formula, _ in decisions))\n",
    "len_class = len_vocab * 2\n",
    "\n",
    "print(\"Loaded {} decisions\".format(len(decisions)))\n",
    "\n",
    "# Split into training/test sets.\n",
    "num_training = int(len(decisions) * TRAIN_SPLIT)\n",
    "decisions_training, decisions_test = decisions[:num_training], decisions[num_training:]\n",
    "\n",
    "def encode_policy(policy) -> int:\n",
    "    \"\"\"Maps a policy tuple to an ordinal class.\"\"\"\n",
    "    idx, branch = policy\n",
    "    zero_idx = idx - 1\n",
    "    return zero_idx * 2 + int(branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kkMlawMd20N"
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vnO9joxa4rhH"
   },
   "source": [
    "Let's look at how the approximate optimal heuristic is distributed. We're expecting a roughly uniform distribution over variable names, as variables are initially named randomly and our renaming is a bijection, preserving its randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "LUQRzf75zTFO",
    "outputId": "a5243042-fdc0-4005-a31a-63c49a349768"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "picked_vars, picked_assignments = zip(*((int(var), int(assignment)) for _, (var, assignment) in decisions))\n",
    "picked_vars, picked_assignments = np.array(picked_vars), np.array(picked_assignments)\n",
    "\n",
    "n, bins, _ = plt.hist(picked_vars, len_vocab)\n",
    "plt.title('Best variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "i7owfeQAzXLl",
    "outputId": "9414bb2d-142c-42ae-d89d-3fb6492c4811"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(picked_assignments, 2)\n",
    "plt.title('Assignments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_iFHcjU4vGk"
   },
   "source": [
    "# LSTM\n",
    "For the LSTM model, we need to convert our dataset of formulas in AST form into a sequence of tokens. We're going to do some preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QpPuavocHRh"
   },
   "source": [
    "## Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImylKus9zYHM"
   },
   "outputs": [],
   "source": [
    "len_token = len_vocab + 2 # for connectives AND and OR\n",
    "\n",
    "def one_hot(length: int, index: int):\n",
    "    element = -1 if index < 0 else 1\n",
    "    index = abs(index)\n",
    "    \n",
    "    v = [0] * length\n",
    "    v[index] = element\n",
    "    return v\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    expression.Type.AND: one_hot(len_token, len_vocab),\n",
    "    expression.Type.OR: one_hot(len_token, len_vocab + 1)\n",
    "}\n",
    "\n",
    "def tokenize_fastcnf(formula):\n",
    "    tokenized = []\n",
    "    for i, clause in enumerate(formula):\n",
    "        # Intersperse conjunct tokens\n",
    "        if i > 0:\n",
    "            tokenized.append(SPECIAL_TOKENS[expression.Type.AND])\n",
    "        \n",
    "        for j, literal in enumerate(clause):\n",
    "            # Intersperse disjunct tokens\n",
    "            if j > 0:\n",
    "                tokenized.append(SPECIAL_TOKENS[expression.Type.OR])\n",
    "            \n",
    "            zero_idx_var = literal - 1\n",
    "            tokenized.append(one_hot(len_token, zero_idx_var))\n",
    "\n",
    "    return torch.FloatTensor(tokenized)\n",
    "\n",
    "def tokenize_formula(formula):\n",
    "    return torch.FloatTensor(_tokenize_formula(formula))\n",
    "\n",
    "def _tokenize_formula(formula):\n",
    "    if not formula:\n",
    "        return []\n",
    "    \n",
    "    assert isinstance(formula, expression.Expression)\n",
    "    \n",
    "    typ, l_val, r_val = formula\n",
    "\n",
    "    # We're at a variable, embed it.\n",
    "    if typ == expression.Type.VAR:\n",
    "        # Variables are 1-indexed\n",
    "        zero_idx = int(l_val) - 1\n",
    "        return [one_hot(len_token, zero_idx)]\n",
    "    # We're at a NOT'd variable, reach one level below and embed it.\n",
    "    if typ == expression.Type.NOT:\n",
    "        assert l_val.typ == expression.Type.VAR, \"Expecting negations to be propagated to literal level\"\n",
    "        zero_idx = int(l_val.l_val) - 1\n",
    "        return [one_hot(len_token, -1 * zero_idx)]\n",
    "\n",
    "    # Else join tokenizations of branches into one sequence\n",
    "    return _tokenize_formula(l_val) + [SPECIAL_TOKENS[typ]] + _tokenize_formula(r_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhdSFNoh4z5r"
   },
   "source": [
    "## Model\n",
    "\n",
    "Now let's define the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_uz4-wkzsjY"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, linear_1_dim: int, linear_2_dim: int, num_lstm_layers: int):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, num_lstm_layers, batch_first=True)\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.hidden_dim, linear_1_dim)\n",
    "        self.linear_2 = nn.Linear(linear_1_dim, linear_2_dim)\n",
    "        \n",
    "    def forward(self, packed_input):\n",
    "        \"\"\"Expects a packed sequence.\"\"\"\n",
    "        _, (hidden, _) = self.lstm(packed_input)\n",
    "        \n",
    "        x = hidden[-1]\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKy96IQ-cP5F"
   },
   "source": [
    "# Graph Encoding\n",
    "\n",
    "For the graph encoding model, we need to convert our dataset of formulas in AST into adjacency matrices.\n",
    "\n",
    "$$M(i, j) = \\mathbb{1}\\{l_i \\in c_j\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig3yzsFkcazp"
   },
   "source": [
    "## Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs7iU5PEcZ8J"
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def adjacency(formulas):\n",
    "    \"\"\"\n",
    "    Takes any number of formulas and returns an adjacency matrix representing the formulas\n",
    "    (disconnected if > 1) formulas.\n",
    "    \"\"\"\n",
    "    if not isinstance(formulas, list):\n",
    "        formulas = [formulas]\n",
    "\n",
    "    # Find how many variables we'll have, so that we can index beyond\n",
    "    # that set for negative literals. This is a running sum.\n",
    "    # 1, 4, 4, 6, 20, 30\n",
    "    num_variables = reduce(lambda l, f: l + [(0 if not l else l[-1]) + len(fast_cnf.free(f))], formulas, [])\n",
    "    num_clauses = 0\n",
    "\n",
    "    # The sparse tensor we're building up\n",
    "    indices = []\n",
    "\n",
    "    for n, formula in enumerate(formulas):\n",
    "        # How many variables have we seen before?\n",
    "        prev_vars = 0 if n == 0 else num_variables[n - 1]\n",
    "\n",
    "        for clause in formula:\n",
    "            for j, literal in enumerate(clause):\n",
    "                clause_index = num_clauses\n",
    "                # Literals are 1 indexed\n",
    "                literal_index = prev_vars + abs(literal) - 1\n",
    "                # Negative literals go after all positive literals\n",
    "                if literal < 0:\n",
    "                    literal_index += num_variables[-1]\n",
    "                \n",
    "                indices.append([literal_index, clause_index])\n",
    "\n",
    "            num_clauses += 1\n",
    "\n",
    "    indices = torch.LongTensor(indices)\n",
    "    values = torch.zeros(len(indices)) + 1\n",
    "    # 2m + n where m is the number of literals, and n is the number of clauses\n",
    "    size = torch.Size([2 * num_variables[-1], num_clauses])\n",
    "    adj_matrix = torch.sparse.FloatTensor(indices.t(), values, size)\n",
    "\n",
    "    return adj_matrix, [len(fast_cnf.free(f)) for f in formulas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7FoXh54T6IS"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aur1Jl2ViS-m"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Multi-layer neural net.\"\"\"\n",
    "    def __init__(self, input_size, output_sizes):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size if i == 0 else output_sizes[i - 1], output_size)\n",
    "            for i, output_size in enumerate(output_sizes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for lin in self.layers:\n",
    "            x = lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GraphEmbeddingLSTM(nn.Module):\n",
    "    def __init__(self, dimension, iterations):\n",
    "        super(GraphEmbeddingLSTM, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # https://github.com/ryanzhangfan/NeuroSAT/blob/master/src/neurosat.py\n",
    "        # We learn the initialization vectors, hence their being linear transformations\n",
    "        # (to make Pytorch happy to learn these vectors)\n",
    "        self.l_init = nn.Linear(1, dimension)\n",
    "        self.c_init = nn.Linear(1, dimension)\n",
    "\n",
    "        # The message nets\n",
    "        self.l_msg = Net(dimension, [dimension, dimension, dimension])\n",
    "        self.c_msg = Net(dimension, [dimension, dimension, dimension])\n",
    "\n",
    "        # The update LSTM\n",
    "        self.l_u = nn.LSTM(dimension, dimension)\n",
    "        self.c_u = nn.LSTM(dimension, dimension)\n",
    "\n",
    "        # The classifier\n",
    "        self.l_cls = Net(dimension, [dimension, dimension, 1])\n",
    "\n",
    "    def forward(self, adj_matrix, num_vars):\n",
    "        # Get our L_init and C_init vectors.\n",
    "        x = torch.ones(1)\n",
    "        x.requires_grad = False\n",
    "        L_init = self.l_init(x)\n",
    "        C_init = self.c_init(x)\n",
    "\n",
    "        # Embedding matrices start out by tiling L_init, C_init.\n",
    "        n_lits, n_clauses = adj_matrix.shape\n",
    "        L_t = L_init.repeat(n_lits, 1)\n",
    "        C_t = C_init.repeat(n_clauses, 1)\n",
    "\n",
    "        # Hidden states for the update LSTMs\n",
    "        L_h = torch.zeros((1, n_lits, self.dimension))\n",
    "        L_0 = torch.zeros((1, n_lits, self.dimension))\n",
    "        C_h = torch.zeros((1, n_clauses, self.dimension))\n",
    "        C_0 = torch.zeros((1, n_clauses, self.dimension))\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            l_agg = adj_matrix.t() @ self.l_msg(L_t)\n",
    "            c_agg = adj_matrix @ self.c_msg(C_t)\n",
    "            \n",
    "            # Clause update uses literal embedding aggregations\n",
    "            _, (C_h, _) = self.c_u(l_agg.unsqueeze(0), (C_h, C_0))\n",
    "            # Literal update uses clause embedding aggregations\n",
    "            _, (L_h, _) = self.l_u(c_agg.unsqueeze(0), (L_h, L_0))\n",
    "\n",
    "            # Unpack the hidden states to go back in the loop\n",
    "            C_t = C_h.squeeze(0)\n",
    "            L_t = L_h.squeeze(0)\n",
    "\n",
    "        # Classify the literal embeddings\n",
    "        L_imp = self.l_cls(L_t)\n",
    "        L_imp = L_imp.squeeze(1)\n",
    "        \n",
    "        # Unpack the batch adjacency matrix\n",
    "        L_imp_unpacked = []\n",
    "        \n",
    "        total_vars = len(L_imp) // 2\n",
    "        currently_seen_vars = 0\n",
    "        for n in enumerate(num_vars):\n",
    "            p_lits = L_imp[currently_seen_vars:currently_seen_vars + n]\n",
    "            n_lits = L_imp[total_vars + currently_seen_vars:total_vars + currently_seen_vars + n]\n",
    "            \n",
    "            L_imp_unpacked.append(p_lits + n_lits)\n",
    "            currently_seen_vars += n\n",
    "            \n",
    "        return torch.FloatTensor(L_imp_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aur1Jl2ViS-m"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Multi-layer neural net.\"\"\"\n",
    "    def __init__(self, input_size, output_sizes):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size if i == 0 else output_sizes[i - 1], output_size)\n",
    "            for i, output_size in enumerate(output_sizes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for lin in self.layers:\n",
    "            x = lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GraphEmbeddingLSTM(nn.Module):\n",
    "    def __init__(self, dimension, iterations):\n",
    "        super(GraphEmbeddingLSTM, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # https://github.com/ryanzhangfan/NeuroSAT/blob/master/src/neurosat.py\n",
    "        # We learn the initialization vectors, hence their being linear transformations\n",
    "        # (to make Pytorch happy to learn these vectors)\n",
    "        self.l_init = nn.Linear(1, dimension)\n",
    "        self.c_init = nn.Linear(1, dimension)\n",
    "\n",
    "        # The message nets\n",
    "        self.l_msg = Net(dimension, [dimension, dimension, dimension])\n",
    "        self.c_msg = Net(dimension, [dimension, dimension, dimension])\n",
    "\n",
    "        # The update LSTM\n",
    "        self.l_u = nn.LSTM(dimension, dimension)\n",
    "        self.c_u = nn.LSTM(dimension, dimension)\n",
    "\n",
    "        # The classifier\n",
    "        self.l_cls = Net(dimension, [dimension, dimension, 1])\n",
    "\n",
    "    def forward(self, adj_matrix, num_vars):\n",
    "        # Get our L_init and C_init vectors.\n",
    "        x = torch.ones(1)\n",
    "        x.requires_grad = False\n",
    "        L_init = self.l_init(x)\n",
    "        C_init = self.c_init(x)\n",
    "\n",
    "        # Embedding matrices start out by tiling L_init, C_init.\n",
    "        n_lits, n_clauses = adj_matrix.shape\n",
    "        L_t = L_init.repeat(n_lits, 1)\n",
    "        C_t = C_init.repeat(n_clauses, 1)\n",
    "\n",
    "        # Hidden states for the update LSTMs\n",
    "        L_h = torch.zeros((1, n_lits, self.dimension))\n",
    "        L_0 = torch.zeros((1, n_lits, self.dimension))\n",
    "        C_h = torch.zeros((1, n_clauses, self.dimension))\n",
    "        C_0 = torch.zeros((1, n_clauses, self.dimension))\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            l_agg = adj_matrix.t() @ self.l_msg(L_t)\n",
    "            c_agg = adj_matrix @ self.c_msg(C_t)\n",
    "            \n",
    "            # Clause update uses literal embedding aggregations\n",
    "            _, (C_h, _) = self.c_u(l_agg.unsqueeze(0), (C_h, C_0))\n",
    "            # Literal update uses clause embedding aggregations\n",
    "            _, (L_h, _) = self.l_u(c_agg.unsqueeze(0), (L_h, L_0))\n",
    "\n",
    "            # Unpack the hidden states to go back in the loop\n",
    "            C_t = C_h.squeeze(0)\n",
    "            L_t = L_h.squeeze(0)\n",
    "\n",
    "        # Classify the literal embeddings\n",
    "        L_imp = self.l_cls(L_t)\n",
    "        L_imp = L_imp.squeeze(1)\n",
    "        \n",
    "        # Unpack the batch adjacency matrix\n",
    "        L_imp_unpacked = []\n",
    "        \n",
    "        total_vars = len(L_imp) // 2\n",
    "        currently_seen_vars = 0\n",
    "        for n in num_vars:\n",
    "            p_lits = L_imp[currently_seen_vars:currently_seen_vars + n]\n",
    "            n_lits = L_imp[total_vars + currently_seen_vars:total_vars + currently_seen_vars + n]\n",
    "            lits = torch.cat((p_lits, n_lits))\n",
    "            \n",
    "            L_imp_unpacked.append(lits)\n",
    "            currently_seen_vars += n\n",
    "            \n",
    "        return torch.stack(L_imp_unpacked)\n",
    "\n",
    "# adj, n_v = adjacency([formula for formula, _ in decisions])\n",
    "\n",
    "# emb = GraphEmbeddingLSTM(40, 3)\n",
    "# emb(adj, n_v).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6clOqjWvPdWC"
   },
   "source": [
    "# Training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WueF3YnoTfnV"
   },
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvFy4_lpUMn2"
   },
   "outputs": [],
   "source": [
    "class LSTMDataset(data.Dataset):\n",
    "    def __init__(self, fastcnf_decisions):\n",
    "        self.fastcnf_decisions = fastcnf_decisions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fastcnf_decisions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        formula, policy = self.fastcnf_decisions[index]\n",
    "        return tokenize_fastcnf(formula), encode_policy(policy)\n",
    "    \n",
    "def lstm_collator(batch):\n",
    "    \"\"\"Expects `batch` to be a list of (X, y) pairs.\"\"\"\n",
    "    X, y = zip(*batch)\n",
    "    seq_lens = torch.tensor([len(seq) for seq in X])\n",
    "\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(X, batch_first=True)\n",
    "    packed_batch = nn.utils.rnn.pack_padded_sequence(\n",
    "        padded_sequences,\n",
    "        seq_lens,\n",
    "        batch_first=True,\n",
    "        enforce_sorted=False\n",
    "    )\n",
    "    \n",
    "    return packed_batch, torch.LongTensor(y)\n",
    "\n",
    "lstm_dataset_train = LSTMDataset(decisions_training)\n",
    "lstm_dataset_val = LSTMDataset(decisions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrseX4fB3M1S"
   },
   "outputs": [],
   "source": [
    "class AdjacencyDataset(data.Dataset):\n",
    "    def __init__(self, fastcnf_decisions):\n",
    "        self.fastcnf_decisions = fastcnf_decisions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fastcnf_decisions)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        formula, policy = self.fastcnf_decisions[index]\n",
    "        return formula, encode_policy(policy)\n",
    "    \n",
    "def adj_collator(batch):\n",
    "    \"\"\"Expects `batch` to be a list of (X, y) pairs.\"\"\"\n",
    "    X, y = zip(*batch)\n",
    "    X = list(X)\n",
    "    \n",
    "    adj, num_vars = adjacency(X)\n",
    "    \n",
    "    return adj.to_dense(), torch.LongTensor(y), num_vars\n",
    "\n",
    "adj_dataset_train = AdjacencyDataset(decisions_training)\n",
    "adj_dataset_val = AdjacencyDataset(decisions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltQeSXfbT9je"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IjSc1fAUt_q"
   },
   "outputs": [],
   "source": [
    "def evaluate_validation(model, loader_validation):\n",
    "    \"\"\"Evaluate a model on the validation set.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "\n",
    "        for batch in loader_validation:\n",
    "            X, y = batch[0], batch[1]\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            y_pred = model(X, *batch[2:])\n",
    "\n",
    "            class_predictions = y_pred.max(1)[1]\n",
    "            num_accurate = class_predictions.eq(y).sum()\n",
    "\n",
    "            accuracy += float(num_accurate) / (y.shape[0] * len(loader_validation))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dataset_training,\n",
    "    dataset_validation,\n",
    "    collate_fn,\n",
    "    batch_size=64,\n",
    "    epochs=150\n",
    "    ):\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "\n",
    "    # Set up the loaders\n",
    "    loader_opts = {\n",
    "        'collate_fn': collate_fn,\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "\n",
    "    loader_train = data.DataLoader(dataset=dataset_training, **loader_opts)\n",
    "    loader_validation = data.DataLoader(dataset=dataset_validation, **loader_opts)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters()) # Use the default LR schedule\n",
    "    print_every = 1\n",
    "\n",
    "    for t in range(epochs):\n",
    "        # These are over the dataset, so we pull them in each epoch\n",
    "        running_loss, running_accuracy, steps_accrued = 0, 0, 0\n",
    "\n",
    "        for step, batch in enumerate(loader_train):\n",
    "            X, y = batch[0], batch[1]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            y_pred = model(X, *batch[2:])\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                steps_accrued += 1\n",
    "                class_predictions = y_pred.max(1)[1]            \n",
    "                num_accurate = class_predictions.eq(y).sum()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_accuracy += float(num_accurate) / y.shape[0]\n",
    "            \n",
    "                if step % print_every == 0:\n",
    "                    print(f\"Epoch {t + 1}/{epochs} \"\n",
    "                        f\"Step {step + 1}/{len(loader_train)} \"\n",
    "                        f\"Train loss: {running_loss / steps_accrued} \"\n",
    "                        f\"Train acc: {running_accuracy / steps_accrued}\")\n",
    "                    \n",
    "        val_acc = evaluate_validation(model, loader_validation)\n",
    "        print(f\"Validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3r9ISA9vU4eP"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVpebNEYTh2V"
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTM(\n",
    "    input_dim=token_length,\n",
    "    hidden_dim=40,\n",
    "    linear_1_dim=60,\n",
    "    linear_2_dim=label_length,\n",
    "    num_lstm_layers=2\n",
    ")\n",
    "\n",
    "train_model(lstm_model, lstm_dataset_train, lstm_dataset_val, lstm_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgRDPhg4U7T1"
   },
   "source": [
    "### Graph Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSNGqaY-U8pj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 Step 1/7577 Train loss: 3.6887664794921875 Train acc: 0.0\n",
      "Epoch 1/150 Step 2/7577 Train loss: 3.689348578453064 Train acc: 0.0078125\n",
      "Epoch 1/150 Step 3/7577 Train loss: 3.689394553502401 Train acc: 0.010416666666666666\n",
      "Epoch 1/150 Step 4/7577 Train loss: 3.6900296211242676 Train acc: 0.0078125\n",
      "Epoch 1/150 Step 5/7577 Train loss: 3.6900628089904783 Train acc: 0.00625\n",
      "Epoch 1/150 Step 6/7577 Train loss: 3.689720312754313 Train acc: 0.010416666666666666\n",
      "Epoch 1/150 Step 7/7577 Train loss: 3.689623083387102 Train acc: 0.013392857142857142\n",
      "Epoch 1/150 Step 8/7577 Train loss: 3.689736396074295 Train acc: 0.015625\n",
      "Epoch 1/150 Step 9/7577 Train loss: 3.689564678404066 Train acc: 0.017361111111111112\n",
      "Epoch 1/150 Step 10/7577 Train loss: 3.689438796043396 Train acc: 0.01875\n",
      "Epoch 1/150 Step 11/7577 Train loss: 3.689337123524059 Train acc: 0.018465909090909092\n",
      "Epoch 1/150 Step 12/7577 Train loss: 3.689213752746582 Train acc: 0.01953125\n",
      "Epoch 1/150 Step 13/7577 Train loss: 3.689319060398982 Train acc: 0.019230769230769232\n",
      "Epoch 1/150 Step 14/7577 Train loss: 3.689314229147775 Train acc: 0.018973214285714284\n",
      "Epoch 1/150 Step 15/7577 Train loss: 3.6893001238505048 Train acc: 0.021875\n",
      "Epoch 1/150 Step 16/7577 Train loss: 3.689215824007988 Train acc: 0.0205078125\n",
      "Epoch 1/150 Step 17/7577 Train loss: 3.6892071471494785 Train acc: 0.021139705882352942\n",
      "Epoch 1/150 Step 18/7577 Train loss: 3.689152571890089 Train acc: 0.021701388888888888\n",
      "Epoch 1/150 Step 19/7577 Train loss: 3.689273144069471 Train acc: 0.022203947368421052\n",
      "Epoch 1/150 Step 20/7577 Train loss: 3.6892730236053466 Train acc: 0.021875\n",
      "Epoch 1/150 Step 21/7577 Train loss: 3.689258654912313 Train acc: 0.023065476190476192\n",
      "Epoch 1/150 Step 22/7577 Train loss: 3.689246242696589 Train acc: 0.0234375\n",
      "Epoch 1/150 Step 23/7577 Train loss: 3.689249733219976 Train acc: 0.022418478260869564\n",
      "Epoch 1/150 Step 24/7577 Train loss: 3.6892627477645874 Train acc: 0.022135416666666668\n",
      "Epoch 1/150 Step 25/7577 Train loss: 3.6892263412475588 Train acc: 0.021875\n",
      "Epoch 1/150 Step 26/7577 Train loss: 3.6892101764678955 Train acc: 0.02283653846153846\n",
      "Epoch 1/150 Step 27/7577 Train loss: 3.68917014863756 Train acc: 0.023148148148148147\n",
      "Epoch 1/150 Step 28/7577 Train loss: 3.689199754170009 Train acc: 0.022321428571428572\n",
      "Epoch 1/150 Step 29/7577 Train loss: 3.689178705215454 Train acc: 0.024245689655172414\n",
      "Epoch 1/150 Step 30/7577 Train loss: 3.689204454421997 Train acc: 0.023958333333333335\n"
     ]
    }
   ],
   "source": [
    "adj_model = GraphEmbeddingLSTM(140, 25)\n",
    "train_model(adj_model, adj_dataset_train, adj_dataset_val, adj_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "L_iFHcjU4vGk"
   ],
   "machine_shape": "hm",
   "name": "sat-ml.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
